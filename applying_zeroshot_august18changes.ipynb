{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import torch # if on apple, must be imported or model will fall to cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/data/kayla_data/')\n",
    "df = pd.read_csv('tweets_final_cleaned_july23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = [\"takes a position\", \"does not take a position\"]\n",
    "hypothesis_template = \"This tweet {} on a political issue\"\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\", device=torch.device('mps'), batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the functions for classification. Saving file every 8000 in case of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, id_col, text_col, hypothesis_template, candidate_labels):\n",
    "    texts = batch[text_col].tolist()  # convert documents in batch to a list\n",
    "    results = classifier(texts, candidate_labels=candidate_labels, hypothesis_template=hypothesis_template, multi_label=False)\n",
    "    \n",
    "    # what we will want for each document\n",
    "    df_results = pd.DataFrame({\n",
    "        'tweet_id': batch[id_col], \n",
    "        'text': texts,  \n",
    "        'predicted_label': [result['labels'][0] for result in results],\n",
    "        'score': [result['scores'][0] for result in results]\n",
    "    })\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# defining function to apply classification\n",
    "def predict_political(df, id_col, text_col, hypothesis_template, candidate_labels, batch_size=8000, checkpoint_dir='checkpoints_large'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    num_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)\n",
    "    \n",
    "    # in case model fails after hours of running, start after most recent batch\n",
    "    existing_files = sorted([f for f in os.listdir(checkpoint_dir) if f.startswith('batch_')])\n",
    "    start_batch = len(existing_files)\n",
    "    \n",
    "    for i in tqdm(range(start_batch, num_batches), desc=\"Processing Batches\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "\n",
    "        batch = df.iloc[start_idx:end_idx]\n",
    "\n",
    "        # process batch\n",
    "        batch_results = process_batch(batch, id_col, text_col, hypothesis_template, candidate_labels)\n",
    "\n",
    "        # save current batch results as df\n",
    "        batch_filename = os.path.join(checkpoint_dir, f'batch_{i}.csv')\n",
    "        batch_results.to_csv(batch_filename, index=False)\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    # concatenate all batch files into a single file at the end\n",
    "    batch_files = sorted([os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.startswith('batch_')])\n",
    "    final_result_df = pd.concat((pd.read_csv(f) for f in batch_files), ignore_index=True)\n",
    "\n",
    "    # save final df to file\n",
    "    final_output_path = 'final_classified_tweets.csv'\n",
    "    final_result_df.to_csv(final_output_path, index=False)\n",
    "    \n",
    "    return final_result_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 144/144 [6:00:25<00:00, 150.18s/it]  \n"
     ]
    }
   ],
   "source": [
    "text_col = 'text'\n",
    "candidate_labels = ['takes a position', 'does not take a position']\n",
    "hypothesis_template = 'This tweet {} on a political issue'\n",
    "checkpoint_dir = '/checkpoints/checkpoints_large_aug'\n",
    "# note: overwriting batch size from above chunk to smaller size\n",
    "final_results = predict_political(df, id_col='tweet_id', text_col=text_col, hypothesis_template=hypothesis_template,candidate_labels=candidate_labels, batch_size=5000, checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is redundant but just in case any data loss save full dataframe from fully run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.to_csv('deblarge_classified_tweets_aug.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('/final_classified_tweets_aug19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431606"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.loc[df['predicted_label'] == 'takes a position'].to_csv('classified_deblarge_positives.csv')\n",
    "len(df.loc[df['predicted_label'] == 'takes a position'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HuggingFaceGuidedTourForMac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
